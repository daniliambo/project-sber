package ru.sberbank.bigdata.enki.plan.converter

import cats.data.State
import cats.instances.vector._
import cats.syntax.applicative._
import cats.syntax.traverse._
import org.apache.spark.sql.catalyst.expressions.{ScalarSubquery => SparkScalarSubquery, _}
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import ru.sberbank.bigdata.enki.plan.columns._
import ru.sberbank.bigdata.enki.plan.expression._
import ru.sberbank.bigdata.enki.plan.nodes._
import ru.sberbank.bigdata.enki.plan.util.NameContext

import scala.annotation.tailrec

package object transformers {

  // State monad is used to make code shorter
  type ContextState[T] = State[NameContext, T]

  val generateName: ContextState[String]    = State(_.generateName)
  val getContext: ContextState[NameContext] = State.get

  def updateName(name: String): ContextState[String] = State(_.update(name))

  def wrapInSelect(source: Node): ContextState[SelectNode] =
    source match {
      case _: JoinNode | _: GeneratorNode => SelectNode(source).pure[ContextState]

      case otherNode => wrapInAliased(otherNode).map(SelectNode(_))
    }

  def wrapInAliased(node: Node): ContextState[AliasedNode] =
    node match {
      case aliasedNode: AliasedNode => aliasedNode.pure[ContextState]

      case otherNode => generateName.map(createAliased(otherNode, _))
    }

  def createAliased(node: Node, alias: String): AliasedNode = {
    val wrapped = node match {
      case _: SelectNode | _: SetOperationNode | _: SourceTableNode | _: InlineTableNode => node
      case other                                                                         => SelectNode(other)
    }

    AliasedNode(alias, wrapped)
  }

  /** Returns all [[SourceColumn]] referenced in this expression */
  def getSourceTableColumns(expression: Expression, references: ReferenceMap): Vector[SourceColumn] =
    expression.references.flatMap(attr => references(attr.exprId.id).sourceColumns).toVector.distinct

  def convertNamedExpressions(
    expressions: Seq[NamedExpression],
    references: ReferenceMap,
    aliasContext: NameContext
  ): Vector[Column] = expressions.map(convertNamedExpression(_, references, aliasContext)).toVector

  /** Convert [[org.apache.spark.sql.catalyst.expressions.NamedExpression NamedExpression]]
    * into [[Column]]
    */
  def convertNamedExpression(namedExpr: NamedExpression, references: ReferenceMap, aliasContext: NameContext): Column =
    namedExpr match {
      case aliasExpr @ Alias(child, alias) =>
        val sourceTableColumns = getSourceTableColumns(child, references)
        val updatedExpression  = updateExpression(child, references, aliasContext)
        ExpressionColumn(aliasExpr.exprId.id, alias, updatedExpression, sourceTableColumns)

      case attribute: Attribute => references(attribute.exprId.id)
    }

  def updateExpression(expression: Expression, references: ReferenceMap, aliasContext: NameContext): Expression = {
    def replaceSubquery(subquery: Expression): Subquery =
      subquery match {
        case scalarSubquery: SparkScalarSubquery =>
          val node = Converter.convertPlan(scalarSubquery.plan, references, aliasContext)
          ScalarSubquery(node, scalarSubquery.dataType)

        case exists: Exists =>
          val node = Converter.convertPlan(exists.plan, references, aliasContext)
          ExistsSubquery(node)

        case In(value, Seq(listQuery: ListQuery)) =>
          val node = Converter.convertPlan(listQuery.plan, references, aliasContext)
          InSubquery(value, node)

        case _ => throw new IllegalStateException("Unreachable")
      }

    def replaceAttribute(attribute: Attribute): AttributeFromColumn = {
      val attrId = attribute.exprId.id
      val column = references(attrId)
      AttributeFromColumn(column)
    }

    expression.transform {
      case attribute: Attribute => replaceAttribute(attribute)

      case scalarSubquery: SparkScalarSubquery => replaceSubquery(scalarSubquery)

      case exists: Exists => replaceSubquery(exists)

      case inSubquery @ In(_, Seq(_: ListQuery)) => replaceSubquery(inSubquery)

      case OuterReference(namedExpr) =>
        namedExpr match {
          case attribute: Attribute => replaceAttribute(attribute)
          case other                => other
        }
    }
  }

  /** Replaces generated by Spark aliases' names with more meaningful ones */
  def renameGeneratedExpressions(
    expressions: Vector[NamedExpression],
    usedNames: Vector[String] = Vector.empty
  ): Vector[NamedExpression] = {
    @tailrec def prettyName(expression: Expression): String =
      expression match {
        case AggregateExpression(aggregateFunction, _, isDistinct, _) =>
          val prefix = if (isDistinct) "distinct_" else ""
          prefix + aggregateFunction.prettyName

        case WindowExpression(windowFunction, _) =>
          windowFunction match {
            case aggregate: AggregateExpression => prettyName(aggregate)

            case _ => windowFunction.prettyName
          }

        case other => other.prettyName
      }

    val context = NameContext.fromNames(usedNames ++ expressions.map(_.name))
    expressions.traverse {
      case alias @ Alias(child, name) if name.startsWith("_w") || child.toString == name =>
        val newName = prettyName(child)
        updateName(newName).map { updatedName =>
          alias.copy(name = updatedName)(alias.exprId, alias.qualifier, alias.explicitMetadata, alias.isGenerated)
        }: ContextState[NamedExpression]

      case other => other.pure[ContextState]
    }
      .runA(context)
      .value
  }

}
